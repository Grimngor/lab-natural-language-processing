{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javi\\AppData\\Local\\Temp\\ipykernel_73012\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speed up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\Javi\\AppData\\Local\\Temp\\ipykernel_73012\\2609927970.py:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  email_df = pd.read_csv(\"E:\\IronHack\\Week 4\\Lab_3 - Natural Language Processing\\lab-natural-language-processing\\data\\kg_train.csv\",encoding='latin-1')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sure -- bottom line - you need a special secur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dear Sir,I am Engr. Ugo Nzego with the Enginee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abedin Huma &lt;AbedinH@state.gov&gt;Saturday Novemb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There is an Oct 16th George Marshall event at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;P&gt;1 25% for you as the account owner &lt;BR&gt;2 65...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0\n",
       "5  sure -- bottom line - you need a special secur...      0\n",
       "6  Dear Sir,I am Engr. Ugo Nzego with the Enginee...      1\n",
       "7  Abedin Huma <AbedinH@state.gov>Saturday Novemb...      0\n",
       "8  There is an Oct 16th George Marshall event at ...      0\n",
       "9  <P>1 25% for you as the account owner <BR>2 65...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "email_df = pd.read_csv(\"E:\\IronHack\\Week 4\\Lab_3 - Natural Language Processing\\lab-natural-language-processing\\data\\kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "#TODO Modify for final system\n",
    "email_df = email_df.head(1000)\n",
    "print(email_df.shape)\n",
    "email_df.fillna(\"\",inplace=True)\n",
    "email_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Features and Target\n",
    "X = email_df[\"text\"]\n",
    "y = email_df[\"label\"]\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javi\\AppData\\Local\\Temp\\ipykernel_73012\\92197393.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(email_text, 'html.parser')\n",
      "C:\\Users\\Javi\\AppData\\Local\\Temp\\ipykernel_73012\\92197393.py:4: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(email_text, 'html.parser')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56     Tom harkin called office twice duringCell:Home:AM\n",
       "955    Dear sir=2C I got your contact in cause of a s...\n",
       "231                                                   18\n",
       "738                               She is on her cell.###\n",
       "740                         Pls keep the updates coming!\n",
       "566    Pis print.H Wednesday. October 17 2012 5.48 PM...\n",
       "681                                                  Fyi\n",
       "463                                                 Yes!\n",
       "866    Sorry - been on calls all day re coordination ...\n",
       "11     Dear Friend,My name is Edward Moore QC.Princip...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def clean_css(email_text):\n",
    "    soup = BeautifulSoup(email_text, 'html.parser')\n",
    "    for style in soup.findAll(attrs={'style': True}):\n",
    "        style.replace_with('')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    for element in soup(text=lambda text: isinstance(text, Comment)):\n",
    "        element.extract()\n",
    "    for tag in soup.find_all():\n",
    "        tag.unwrap()\n",
    "        \n",
    "    return str(soup)\n",
    "\n",
    "X_train_no_css = X_train.apply(clean_css)\n",
    "X_test_no_css = X_test.apply(clean_css)\n",
    "X_train_no_css.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56     tom harkin called office twice duringcell home am\n",
       "955    dear sir got your contact in cause of seriouse...\n",
       "231                                                     \n",
       "738                                   she is on her cell\n",
       "740                          pls keep the updates coming\n",
       "566                    pis print wednesday october pmbfw\n",
       "681                                                  fyi\n",
       "463                                                  yes\n",
       "866    sorry been on calls all day re coordination fo...\n",
       "11     dear friend my name is edward moore qc princip...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "        \n",
    "    text = ''.join(f' {e} ' if e in string.punctuation else e for e in text)  # Add spaces around special characters\n",
    "    text = ''.join(e for e in text if e.isalpha() or e.isspace())  # Remove special characters and numbers\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)  # Remove single characters\n",
    "    text = text.lstrip()  # Remove single characters from the start\n",
    "    text = ' '.join(text.split())  # Substitute multiple spaces with single space\n",
    "    text = text.replace('b ', '')  # Remove prefixed 'b'\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "X_train_clean = X_train_no_css.apply(clean_text)\n",
    "X_test_clean = X_test_no_css.apply(clean_text)\n",
    "X_train_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56        tom harkin called office twice duringcell home\n",
       "955    dear sir got contact cause seriouse search rel...\n",
       "231                                                     \n",
       "738                                                 cell\n",
       "740                              pls keep updates coming\n",
       "566                    pis print wednesday october pmbfw\n",
       "681                                                  fyi\n",
       "463                                                  yes\n",
       "866            sorry calls day coordination haitihow sun\n",
       "11     dear friend name edward moore qc principal par...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))  # Get the English stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "X_train_no_sw = X_train_clean.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "X_test_no_sw = X_test_clean.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "X_train_no_sw.head(10)\n",
    "\n",
    "# Removing rows that have became empty after pre-processing\n",
    "#X_train_no_sw = X_train_no_sw[X_train_no_sw != '']\n",
    "#X_test_no_sw = X_test_no_sw[X_test_no_sw != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN) # returns the word type (Noun if we have not found)\n",
    "\n",
    "def lemmatize_email(text):\n",
    "\n",
    "    # tokenize the text by splitting on spaces\n",
    "    tokens = text.split()\n",
    "\n",
    "    # lemmatize tokens\n",
    "    lem_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "\n",
    "    return ' '.join(lem_tokens)\n",
    "\n",
    "X_train_lem = X_train_no_sw.apply(lemmatize_email)\n",
    "X_test_lem = X_test_no_sw.apply(lemmatize_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing X_train using Bag of Words with CountVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train_lem)\n",
    "#X_test_bow = bow_vectorizer.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money: 795\n",
      "account: 697\n",
      "bank: 634\n",
      "fund: 593\n",
      "mr: 478\n",
      "country: 451\n",
      "transaction: 436\n",
      "business: 433\n",
      "transfer: 428\n",
      "million: 362\n"
     ]
    }
   ],
   "source": [
    "# Counting word frequencies\n",
    "word_freqs = X_train_bow.toarray().sum(axis=0)\n",
    "\n",
    "# Sort words by frequency in descending order\n",
    "sorted_words = sorted(zip(\n",
    "    bow_vectorizer.get_feature_names_out(), word_freqs), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Print the top 10 words\n",
    "for word, freq in sorted_words[:10]:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([X_train_lem, y_train], axis=1)\n",
    "data_val = pd.concat([X_test_lem, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "With CountVectorizer a matrix is created where each row represents a sentence,\\\n",
    "    and each column represents a unique word (or token) in the corpus.\n",
    "Then CountVectorizer counts the frequency of each word in each sentence\\\n",
    "    and stores it in the matrix.\n",
    "The resulting matrix represents the sentences as numerical vectors, where each element\\\n",
    "    in the vector corresponds to the frequency of a particular word in the document.\n",
    "    This allows for the comparison and analysis of text using various machine learning algorithms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 76282)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the dataset using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lem)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_lem)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000).fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
